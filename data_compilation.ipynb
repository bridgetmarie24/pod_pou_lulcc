{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPH4DrClVdzfmtnB+et+ZKd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Compile pre-processing outputs into one file**\n","\n","By Bridget Bittmann\n","\n","Date created: 04/04/2022\n","\n","Date modified: 06/01/2022"],"metadata":{"id":"bYPQ0sdMguzq"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"0fYb2dkrd2JV","executionInfo":{"status":"ok","timestamp":1666815541688,"user_tz":360,"elapsed":409,"user":{"displayName":"Bridget Bittmann","userId":"07833657036831444235"}}},"outputs":[],"source":["## --------------- ##\n","## IMPORT PACKAGES ## \n","## --------------- ##\n","\n","import pandas as pd # to work with dataframe\n","import os # for file paths\n","import glob # read in a folder of csv\n","import numpy as np # basic statistics\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25841,"status":"ok","timestamp":1666815567525,"user":{"displayName":"Bridget Bittmann","userId":"07833657036831444235"},"user_tz":360},"id":"NYmVxKSWeRUX","outputId":"98d01211-fb15-44f0-928c-bc543c3cb69d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive \n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":413,"status":"ok","timestamp":1666815567929,"user":{"displayName":"Bridget Bittmann","userId":"07833657036831444235"},"user_tz":360},"id":"HmktJEieewNB","outputId":"864464db-ebf4-4c1f-e5e8-e9332dcafe46"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/spatial_colab/datasets\n","\u001b[0m\u001b[01;34mclimate_stats\u001b[0m/         \u001b[01;34mirrigation_companies\u001b[0m/  \u001b[01;34mlcmap_files\u001b[0m/   \u001b[01;34msubset_test_shp\u001b[0m/\n","\u001b[01;34mdiversion_timeseries\u001b[0m/  \u001b[01;34mirrig_lbrb\u001b[0m/            \u001b[01;34mmasked\u001b[0m/\n","\u001b[01;34mextra_figures\u001b[0m/         \u001b[01;34mIrrMapper\u001b[0m/             \u001b[01;34moutput_files\u001b[0m/\n","\u001b[01;34mhydromet_data\u001b[0m/         \u001b[01;34mLBRB_shp\u001b[0m/              \u001b[01;34mPOUs\u001b[0m/\n"]}],"source":["## NAVIGATE TO YOUR DIRECTORY ##\n","%cd gdrive/MyDrive/spatial_colab/datasets/\n","%ls"]},{"cell_type":"code","source":["## -------------------- ##\n","## IMOPORT ALL DATASETS ##\n","## -------------------- ##\n","\n","div_files = pd.read_csv('diversion_timeseries/final_stats/model_inputs_102622.csv')\n","land_files = sorted(glob.glob('lcmap_files/final_metrics/*.csv'))\n","land_bpbc = sorted(glob.glob('lcmap_files/final_metrics/bpbc/*.csv'))\n","climate_files = sorted(glob.glob('climate_stats/final/*.csv'))\n","climate_bpbc = sorted(glob.glob('climate_stats/bpbc_final/*.csv'))\n","hydromet = pd.read_csv('hydromet_data/mode_input_hydromet.csv')\n","common_name_flow = pd.read_csv('diversion_timeseries/relates/name_dictionary_flow.csv')\n","common_name_spatial = pd.read_csv('diversion_timeseries/relates/name_dictionary_spatial.csv')\n","POUSize = pd.read_csv('diversion_timeseries/relates/POUSize.csv')\n","storage = pd.read_csv('diversion_timeseries/storage_100322/WRA_BoiseBasin.csv')\n","quantiles = pd.read_csv('diversion_timeseries/final_stats/accounting/quantiles.csv')"],"metadata":{"id":"xPt2rGYjt5DF","executionInfo":{"status":"ok","timestamp":1666817528192,"user_tz":360,"elapsed":1332,"user":{"displayName":"Bridget Bittmann","userId":"07833657036831444235"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["## --------------------------------- ##\n","## CREATE COMMON NAME TO MERGE FILES ## \n","## --------------------------------- ##\n","\n","# Import discharge data \n","\n","# div_data = []\n","# for i in div_files:\n","#   discharge = pd.read_csv(i)\n","#   discharge = discharge.rename({'Name':'DivName'}, axis=1)\n","#   discharge['DivName'] = discharge['DivName'].str.replace(r\"\\(.*\\)\", \"\")\n","#   div_data.append(discharge)\n","\n","# bpbc = pd.read_csv('diversion_timeseries/bpbc/bpbc_totals.csv')\n","# bpbc = bpbc.rename({'Acre-feet':'Acre_feet'},axis=1)\n","div = div_files\n","# div_bpbc = pd.concat([div,bpbc])\n","# div_bpbc = div_bpbc.iloc[:,[1,2,3]]\n","# Dicharge data dictionary\n","\n","div_dict = dict(zip(common_name_flow['DiversionName'], common_name_flow['NewName']))\n","div['NewName'] = div['Name'].map(div_dict)\n","div = div.drop(['Unnamed: 0', 'Name'], axis=1).rename({'NewName' : 'Name'}, axis = 1)\n","# quantiles['NewName'] = quantiles['Name'].map(div_dict)\n","# quantiles = quantiles.drop('Name', axis=1).rename({'NewName' : 'Name'}, axis = 1)\n","# quantiles = quantiles[quantiles['Quantiles'] != 0]\n","# div = quantiles.merge(div, left_on = ['Name', 'Year'], right_on = ['Name', 'Year'])\n","\n","# Import land use change data\n","\n","land_data = []\n","ld_bpbc = []\n","for i in land_files:\n","  land_data.append(pd.read_csv(i))\n","for i in land_bpbc:\n","  ld_bpbc.append(pd.read_csv(i))\n","\n","land_bpbc = pd.concat(ld_bpbc)\n","land = pd.concat(land_data)\n","\n","# Land use dictionary\n","\n","shape_dict = dict(zip(common_name_spatial['WaterRight'], common_name_spatial['NewName']))\n","land['Name'] = land['DivName'].map(shape_dict)\n","land = land.drop(['Unnamed: 0', 'DivName'], axis=1)\n","POUSize['Name'] = POUSize['WaterRight'].map(shape_dict)\n","POUSize = POUSize.drop(['WaterRight'], axis=1)\n","# Import climate zonal stats\n","\n","clim_data = []\n","clim_bpbc = []\n","for i in climate_files:\n","  clim_data.append(pd.read_csv(i))\n","# for i in climate_bpbc:\n","#   clim_bpbc.append(pd.read_csv(i))\n","clim = pd.concat(clim_data)\n","# climate_bpbc = pd.concat(clim_bpbc)\n","\n","# Use shapefile dictionary on climate data\n","\n","clim['Name'] = clim['DIV_NAME'].map(shape_dict)\n","clim = clim.drop(['Unnamed: 0', 'DIV_NAME'], axis=1)"],"metadata":{"id":"M9yxSzfYu9iH","executionInfo":{"status":"ok","timestamp":1666817552271,"user_tz":360,"elapsed":811,"user":{"displayName":"Bridget Bittmann","userId":"07833657036831444235"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["## -------------------------------------- ## \n","## MERGE DIVERSION DATA TO MATCH THE POUS ## \n","## -------------------------------------- ## \n","\n","\n","# Create function to do this\n","\n","def merge_flows(data, name):\n","  '''\n","  This function will merge two different flow datasets into one for completely overlapping POUs.\n","\n","  Variables:\n","  data : The full diversion dataset\n","  name : A string of the new name for each POU.\n","  '''\n","  old_df = data[data['Name']== name].reset_index().drop('index', axis=1)\n","  new_df = pd.DataFrame()\n","  new_df['Year'] = old_df['Year'].unique()\n","  new_df['Name'] = old_df['Name'][0:34]\n","  sums = old_df.groupby('Year').sum().reset_index()\n","  new_df['Diversion (cfs)'] = sums['Diversion (cfs)']\n","  new_df['Acre_feet'] = sums['Acre_feet']\n","  # quants = old_df['Quantiles'].groupby('Year').max().reset_index()\n","\n","  startday = []\n","  start_date = []\n","  endday = []\n","  range = []\n","  end_date = []\n","\n","  for i in new_df['Year']:\n","    yearly = old_df[old_df['Year'] == i]\n","    start = np.min(yearly['StartDayofYear'].values)\n","    startdate = yearly['StartDate'][yearly['StartDayofYear']==start].values\n","    end = np.max(yearly['EndDayofYear'].values)\n","    enddate = yearly['EndDate'][yearly['EndDayofYear']==end].values\n","    startday.append(start)\n","    endday.append(end)\n","    range.append(end-start)\n","    start_date.append(startdate[0])\n","    end_date.append(enddate[0])\n","\n","  new_df['StartDate'] = start_date\n","  new_df['StartDayofYear'] = startday\n","  new_df['EndDate'] = end_date\n","  new_df['EndDayofYear'] = endday\n","  new_df['Range'] = range\n","\n","  return new_df"],"metadata":{"id":"GI4RA6_iVuIu","executionInfo":{"status":"ok","timestamp":1666817555808,"user_tz":360,"elapsed":327,"user":{"displayName":"Bridget Bittmann","userId":"07833657036831444235"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["## ------------------------------------- ##\n","## MERGE DIVERSION DATASET WITH NEW DATA ##\n","## ------------------------------------- ##\n","\n","# Create a list of names that have completely shared POUs\n","merge_names = ['Shipley and Wagner Pumps', 'Rossi Mill and Meeves Canals', 'Boise City Parks']\n","\n","merged = []\n","for i in merge_names:\n","  new = merge_flows(div, i)\n","  div = div[div['Name'] != i] #Remove old dataframes from full dataset\n","  div = pd.concat([div, new])\n","  merged.append(new)\n","\n","div = div.sort_values(by=['Name', 'Year']).reset_index().drop('index',axis=1)"],"metadata":{"id":"7QnLnDRrW4lR","executionInfo":{"status":"ok","timestamp":1666817561347,"user_tz":360,"elapsed":347,"user":{"displayName":"Bridget Bittmann","userId":"07833657036831444235"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["## BPBC Merging ##\n","\n","relates = pd.read_csv('diversion_timeseries/bpbc/bpbc_relate.csv')\n","\n","# Dicharge data dict\n","key_list = list(bpbc['DiversionNa'])\n","dict_lookup = dict(zip(relates['Discharge'], relates['NewName']))\n","bpbc['Name'] = [dict_lookup[item] for item in key_list]\n","\n","# Land use change dict\n","key_list2 = list(land_bpbc['DivName'])\n","dict_lookup2 = dict(zip(relates['Shape'], relates['NewName']))\n","land_bpbc['Name'] = [dict_lookup2[item] for item in key_list2]\n","\n","key_list3 = list(climate_bpbc['DIV_NAME'])\n","dict_lookup3 = dict(zip(relates['Shape'], relates['NewName']))\n","climate_bpbc['Name'] = [dict_lookup2[item] for item in key_list2]\n","\n","## Flow data\n","bpbc = bpbc.drop('DiversionNa', axis=1)\n","div_bpbc = pd.concat([div, bpbc])\n","all_div = pd.DataFrame(div_bpbc[['Year', 'Name', 'Acre_feet']])\n","all_div = all_div.sort_values(by=['Name', 'Year'])\n","\n","## Land use data\n","land_bpbc = land_bpbc.drop(['Unnamed: 0', 'DivName'], axis=1)\n","all_land = pd.concat([land_bpbc,land])\n","\n","## Climate data\n","climate_bpbc = climate_bpbc.drop(['Unnamed: 0','DIV_NAME'], axis=1)\n","all_clim = pd.concat([climate_bpbc, clim])"],"metadata":{"id":"yyXtaRO5m71A","colab":{"base_uri":"https://localhost:8080/","height":240},"executionInfo":{"status":"error","timestamp":1664898602845,"user_tz":360,"elapsed":955,"user":{"displayName":"Bridget Bittmann","userId":"07833657036831444235"}},"outputId":"ad5b5579-4f6f-47d7-eef8-61f0d782311b"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-9bfeb4a27290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Dicharge data dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mkey_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpbc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DiversionNa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdict_lookup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Discharge'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NewName'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbpbc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdict_lookup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'bpbc' is not defined"]}]},{"cell_type":"code","source":["## ------------------------------- ## \n","## MERGE THREE FILES INTO ONE FILE ##\n","## ------------------------------- ## \n","\n","land_div = div.merge(land, left_on=['Year', 'Name'], right_on=['dates','Name'], how='left')\n","full_df = land_div.merge(clim, left_on=['Year','Name'], right_on=['Year', 'Name'], how='left').sort_values(by=['Name', 'Year'])\n","full_df = full_df.merge(hydromet, left_on='Year', right_on='Year', how='left').drop(['Unnamed: 0', 'dates'], axis=1)\n","full_df = full_df.merge(POUSize, left_on = 'Name', right_on = 'Name', how = 'left')\n","full_df = full_df.merge(storage, left_on = ['Year', 'Name'], right_on = ['Year', 'Name'])\n","# full_df = quantiles.merge(full_df, left_on = ['Year', 'Name'], right_on = ['Year', 'Name'])\n","## --------------------------------------- ##\n","## Export the full csv file for model in R ##\n","## --------------------------------------- ## \n","\n","# Full dataframe export\n","out_path = 'output_files/merged/model_input_102622.csv'\n","full_df.to_csv(out_path)\n","\n","# Individual dataframe export\n","\n","names = full_df['Name'].unique()\n","for i in names:\n","  df = full_df[full_df['Name'] == i]\n","  out_path = os.path.join('output_files/'+i+'.csv')\n","  df.to_csv(out_path)"],"metadata":{"id":"xfC-BYJJtULS","executionInfo":{"status":"ok","timestamp":1666817651891,"user_tz":360,"elapsed":844,"user":{"displayName":"Bridget Bittmann","userId":"07833657036831444235"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["## ------------------------------------ ## \n","## MERGE THREE FILES INTO ONE FILE BPBC ##\n","## ------------------------------------ ## \n","\n","land_div = all_div.merge(all_land, left_on=['Year', 'Name'], right_on=['dates','Name'], how='left')\n","full_df = land_div.merge(all_clim, left_on=['Year','Name'], right_on=['Year', 'Name'], how='left').sort_values(by=['Name', 'Year'])\n","full_df = full_df.merge(hydromet, left_on='Year', right_on='Year', how='left').drop(['Unnamed: 0', 'dates'], axis=1)\n","\n","# Get rid of New York data because using BPBC data\n","full_df = full_df[full_df['Name'] != 'New York Canal']\n","print(full_df['Name'].unique())\n","display(full_df)\n","## --------------------------------------- ##\n","## Export the full csv file for model in R ##\n","## --------------------------------------- ## \n","\n","# Full dataframe export\n","out_path = 'output_files/merged/bpbc_model_input.csv'\n","full_df.to_csv(out_path)"],"metadata":{"id":"Dc6hHQCephil"},"execution_count":null,"outputs":[]}]}